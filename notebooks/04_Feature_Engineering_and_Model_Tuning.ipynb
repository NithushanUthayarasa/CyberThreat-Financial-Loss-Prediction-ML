{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f3ba16-ca61-4a4b-9845-0245468ce9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset shape: (3000, 10)\n",
      "Train shape: (2400, 9) Test shape: (600, 9)\n",
      "✅ Selected High threshold (Million $): 56.28 with Macro F1 0.4949\n",
      "Train class distribution: [1345 1055]\n",
      "Test class distribution: [330 270]\n",
      "\n",
      "Tuning probability threshold for RandomForest...\n",
      "→ RandomForest: selected prob cutoff 0.45 with val Macro F1 0.5225\n",
      "Training RandomForest on full training set...\n",
      "RandomForest → Accuracy: 0.5233 | Macro F1: 0.5168 | Macro Recall: 0.5168 | Cutoff: 0.45\n",
      "Confusion Matrix:\n",
      " [[192 138]\n",
      " [148 122]]\n",
      "\n",
      "Tuning probability threshold for ExtraTrees...\n",
      "→ ExtraTrees: selected prob cutoff 0.45 with val Macro F1 0.5437\n",
      "Training ExtraTrees on full training set...\n",
      "ExtraTrees → Accuracy: 0.4850 | Macro F1: 0.4800 | Macro Recall: 0.4800 | Cutoff: 0.45\n",
      "Confusion Matrix:\n",
      " [[175 155]\n",
      " [154 116]]\n",
      "\n",
      "Tuning probability threshold for XGBoost...\n",
      "→ XGBoost: selected prob cutoff 0.40 with val Macro F1 0.5084\n",
      "Training XGBoost on full training set...\n",
      "XGBoost → Accuracy: 0.5300 | Macro F1: 0.5296 | Macro Recall: 0.5323 | Cutoff: 0.40\n",
      "Confusion Matrix:\n",
      " [[168 162]\n",
      " [120 150]]\n",
      "\n",
      "Tuning probability threshold for LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 864, number of negative: 1056\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000427 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 880\n",
      "[LightGBM] [Info] Number of data points in the train set: 1920, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "→ LightGBM: selected prob cutoff 0.55 with val Macro F1 0.5183\n",
      "Training LightGBM on full training set...\n",
      "[LightGBM] [Info] Number of positive: 1055, number of negative: 1345\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000439 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 880\n",
      "[LightGBM] [Info] Number of data points in the train set: 2400, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "LightGBM → Accuracy: 0.5300 | Macro F1: 0.5104 | Macro Recall: 0.5152 | Cutoff: 0.55\n",
      "Confusion Matrix:\n",
      " [[219 111]\n",
      " [171  99]]\n",
      "\n",
      "Tuning probability threshold for CatBoost...\n",
      "→ CatBoost: selected prob cutoff 0.50 with val Macro F1 0.5374\n",
      "Training CatBoost on full training set...\n",
      "CatBoost → Accuracy: 0.5450 | Macro F1: 0.5270 | Macro Recall: 0.5308 | Cutoff: 0.50\n",
      "Confusion Matrix:\n",
      " [[222 108]\n",
      " [165 105]]\n",
      "\n",
      "✅ STEP 4 COMPLETED SUCCESSFULLY\n",
      "Results saved to: C:\\Users\\uthay\\Desktop\\CyberThreats_FinancialLoss_Prediction_ML_Final_Project\\data\\processed\\step4_binary_prob_tuned.csv\n",
      "Models saved to: C:\\Users\\uthay\\Desktop\\CyberThreats_FinancialLoss_Prediction_ML_Final_Project\\models\n",
      "Chosen probability cutoffs saved to: C:\\Users\\uthay\\Desktop\\CyberThreats_FinancialLoss_Prediction_ML_Final_Project\\data\\processed\\step4_prob_cutoffs.csv\n"
     ]
    }
   ],
   "source": [
    "# Step4_binary_prob_tuned_fixed.py\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\n",
    "\n",
    "# ---------------------------\n",
    "# Suppress warnings\n",
    "# ---------------------------\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------------------------\n",
    "# Base directory (updated)\n",
    "# ---------------------------\n",
    "BASE_DIR = r\"C:\\Users\\uthay\\Desktop\\CyberThreats_FinancialLoss_Prediction_ML_Final_Project\"\n",
    "\n",
    "RAW_DATA_PATH = os.path.join(BASE_DIR, \"data\", \"raw\", \"Global_Cybersecurity_Threats_2015-2024 (1).csv\")\n",
    "MODEL_PATH = os.path.join(BASE_DIR, \"models\")\n",
    "PROCESSED_PATH = os.path.join(BASE_DIR, \"data\", \"processed\")\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "os.makedirs(PROCESSED_PATH, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ---------------------------\n",
    "# Load raw dataset\n",
    "# ---------------------------\n",
    "df = pd.read_csv(RAW_DATA_PATH)\n",
    "print(\"Loaded dataset shape:\", df.shape)\n",
    "\n",
    "# ---------------------------\n",
    "# Target & features\n",
    "# ---------------------------\n",
    "target_col = \"Financial Loss (in Million $)\"\n",
    "X_raw = df.drop(columns=[target_col])\n",
    "y_raw = df[target_col].values\n",
    "\n",
    "# ---------------------------\n",
    "# Train-test split (no leakage)\n",
    "# ---------------------------\n",
    "X_train, X_test, y_train_raw, y_test_raw = train_test_split(\n",
    "    X_raw, y_raw, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "\n",
    "# ---------------------------\n",
    "# Feature engineering\n",
    "# ---------------------------\n",
    "numeric_features = ['Number of Affected Users', 'Incident Resolution Time (in Hours)']\n",
    "categorical_features = ['Attack Type', 'Target Industry', 'Attack Source', 'Security Vulnerability Type']\n",
    "\n",
    "for df_ in [X_train, X_test]:\n",
    "    df_['Users_per_Hour'] = df_['Number of Affected Users'] / (df_['Incident Resolution Time (in Hours)'] + 1)\n",
    "    df_['Log_Users'] = np.log1p(df_['Number of Affected Users'])\n",
    "\n",
    "numeric_features_ext = numeric_features + ['Users_per_Hour', 'Log_Users']\n",
    "\n",
    "# ---------------------------\n",
    "# Preprocessing pipeline\n",
    "# ---------------------------\n",
    "numeric_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipe, numeric_features_ext),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "])\n",
    "\n",
    "# ---------------------------\n",
    "# Threshold tuning for binary target\n",
    "# ---------------------------\n",
    "X_tr_sub, X_val_sub, y_tr_sub_raw, y_val_sub_raw = train_test_split(\n",
    "    X_train, y_train_raw, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "best_thr, best_f1 = None, -1\n",
    "for perc in [40, 45, 50, 55, 60, 65, 70]:\n",
    "    thr = np.percentile(y_tr_sub_raw, perc)\n",
    "    y_tr_bin = (y_tr_sub_raw > thr).astype(int)\n",
    "    y_val_bin = (y_val_sub_raw > thr).astype(int)\n",
    "\n",
    "    rf_tmp = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE, class_weight=\"balanced\")\n",
    "    pipe = Pipeline([('pre', preprocessor), ('clf', rf_tmp)])\n",
    "    pipe.fit(X_tr_sub, y_tr_bin)\n",
    "    preds_val = pipe.predict(X_val_sub)\n",
    "    f1 = f1_score(y_val_bin, preds_val, average='macro')\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_thr = f1, thr\n",
    "\n",
    "print(f\"✅ Selected High threshold (Million $): {best_thr:.2f} with Macro F1 {best_f1:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Create binary target\n",
    "# ---------------------------\n",
    "y_train = (y_train_raw > best_thr).astype(int)\n",
    "y_test = (y_test_raw > best_thr).astype(int)\n",
    "print(\"Train class distribution:\", np.bincount(y_train))\n",
    "print(\"Test class distribution:\", np.bincount(y_test))\n",
    "\n",
    "# ---------------------------\n",
    "# Models\n",
    "# ---------------------------\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=500, random_state=RANDOM_STATE, class_weight=\"balanced\"),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(n_estimators=500, random_state=RANDOM_STATE, class_weight=\"balanced\"),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=800, max_depth=6, learning_rate=0.03,\n",
    "                             use_label_encoder=False, eval_metric='logloss',\n",
    "                             random_state=RANDOM_STATE),\n",
    "    \"LightGBM\": LGBMClassifier(n_estimators=800, learning_rate=0.03, random_state=RANDOM_STATE, class_weight=\"balanced\"),\n",
    "    \"CatBoost\": CatBoostClassifier(iterations=900, depth=8, learning_rate=0.05,\n",
    "                                   loss_function=\"Logloss\", verbose=0, random_seed=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "# ---------------------------\n",
    "# Preprocess full training data\n",
    "# ---------------------------\n",
    "X_train_final = X_train[numeric_features_ext + categorical_features]\n",
    "X_test_final = X_test[numeric_features_ext + categorical_features]\n",
    "\n",
    "X_train_proc = preprocessor.fit_transform(X_train_final)\n",
    "X_test_proc = preprocessor.transform(X_test_final)\n",
    "\n",
    "# Save preprocessor\n",
    "joblib.dump(preprocessor, os.path.join(MODEL_PATH, \"preprocessor_step4_binary.joblib\"))\n",
    "\n",
    "# ---------------------------\n",
    "# Probability threshold tuning function\n",
    "# ---------------------------\n",
    "def tune_prob_threshold(model, X_tr, y_tr, X_val, y_val, name=\"model\"):\n",
    "    model.fit(X_tr, y_tr)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        val_probs = model.predict_proba(X_val)[:, 1]\n",
    "    else:  # fallback for models without predict_proba\n",
    "        val_probs = (model.decision_function(X_val) - model.decision_function(X_val).min()) / \\\n",
    "                    (model.decision_function(X_val).max() - model.decision_function(X_val).min() + 1e-9)\n",
    "\n",
    "    best_cut, best_f1_score = 0.5, -1\n",
    "    for cut in np.linspace(0.2, 0.8, 13):\n",
    "        preds_val = (val_probs > cut).astype(int)\n",
    "        f1 = f1_score(y_val, preds_val, average='macro')\n",
    "        if f1 > best_f1_score:\n",
    "            best_f1_score = f1\n",
    "            best_cut = cut\n",
    "    print(f\"→ {name}: selected prob cutoff {best_cut:.2f} with val Macro F1 {best_f1_score:.4f}\")\n",
    "    return best_cut, best_f1_score\n",
    "\n",
    "# ---------------------------\n",
    "# Tune, train, and evaluate models\n",
    "# ---------------------------\n",
    "results = {}\n",
    "chosen_cutoffs = {}\n",
    "\n",
    "X_train_sub_proc, X_val_sub_proc, y_train_sub, y_val_sub = train_test_split(\n",
    "    X_train_proc, y_train, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTuning probability threshold for {name}...\")\n",
    "    cut, val_f1 = tune_prob_threshold(model, X_train_sub_proc, y_train_sub, X_val_sub_proc, y_val_sub, name=name)\n",
    "    chosen_cutoffs[name] = cut\n",
    "\n",
    "    print(f\"Training {name} on full training set...\")\n",
    "    model.fit(X_train_proc, y_train)\n",
    "\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        test_probs = model.predict_proba(X_test_proc)[:, 1]\n",
    "    else:\n",
    "        test_probs = (model.decision_function(X_test_proc) - model.decision_function(X_test_proc).min()) / \\\n",
    "                     (model.decision_function(X_test_proc).max() - model.decision_function(X_test_proc).min() + 1e-9)\n",
    "\n",
    "    preds = (test_probs > cut).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds, average='macro')\n",
    "    rec = recall_score(y_test, preds, average='macro')\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "\n",
    "    print(f\"{name} → Accuracy: {acc:.4f} | Macro F1: {f1:.4f} | Macro Recall: {rec:.4f} | Cutoff: {cut:.2f}\")\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    results[name] = {\"Accuracy\": acc, \"Macro_F1\": f1, \"Macro_Recall\": rec, \"Cutoff\": cut}\n",
    "    joblib.dump(model, os.path.join(MODEL_PATH, f\"{name}_step4_binary_prob_tuned.joblib\"))\n",
    "\n",
    "# ---------------------------\n",
    "# Save results\n",
    "# ---------------------------\n",
    "results_df = pd.DataFrame(results).T.reset_index().rename(columns={\"index\": \"Model\"})\n",
    "results_df.to_csv(os.path.join(PROCESSED_PATH, \"step4_binary_prob_tuned.csv\"), index=False)\n",
    "\n",
    "cutoffs_df = pd.Series(chosen_cutoffs, name=\"Cutoff\").reset_index().rename(columns={\"index\": \"Model\"})\n",
    "cutoffs_df.to_csv(os.path.join(PROCESSED_PATH, \"step4_prob_cutoffs.csv\"), index=False)\n",
    "\n",
    "print(\"\\n✅ STEP 4 COMPLETED SUCCESSFULLY\")\n",
    "print(\"Results saved to:\", os.path.join(PROCESSED_PATH, \"step4_binary_prob_tuned.csv\"))\n",
    "print(\"Models saved to:\", MODEL_PATH)\n",
    "print(\"Chosen probability cutoffs saved to:\", os.path.join(PROCESSED_PATH, \"step4_prob_cutoffs.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5845b614-6276-4f94-89f7-93a33430fde5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
