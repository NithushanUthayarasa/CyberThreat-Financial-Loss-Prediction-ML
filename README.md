# ğŸ” Cyber Threats & Financial Loss Prediction (2015â€“2024)

An end-to-end machine learning project predicting high financial losses from cyber threats using structured, data-leak-free global incident data. The project implements a complete ML pipeline, from preprocessing to deployment of the best-performing model for real-world predictions.

---

## ğŸ“Œ Project Overview

Cybersecurity incidents increasingly cause significant financial losses across industries. Estimating losses is challenging due to complex, non-linear factors such as attack type, affected industry, incident resolution time, and number of affected users.

**This project provides:**

* High-risk cyber incident prediction (binary classification)
* Key feature identification driving financial loss
* Model benchmarking for robust predictions
* Deployment of the best model for production use

---

## ğŸš¨ Problem Statement

Organizations often struggle to quantify financial loss from cyber attacks because of:

* Rapidly evolving cyber threat types
* Variability in incident resolution times
* Industry-specific vulnerabilities and defenses

**Solution:** Reframe as binary classification using a high-loss threshold:

* **High Loss (1):** Incidents above optimized threshold (maximizes Macro F1)  
* **Low/Moderate Loss (0):** Incidents below or equal to threshold  

_No separate low-loss threshold is required._

---

## ğŸ¯ Purpose

Provide organizations with a risk prediction system to:

* Prioritize high-risk cyber incidents
* Allocate cybersecurity resources efficiently
* Reduce financial and operational impact
* Support data-driven cybersecurity investments

---

## ğŸ§¾ Dataset & Features

* **Source:** Kaggle â€“ Global Cybersecurity Threats (2015â€“2024)  
* **Size:** ~3,000 incidents  

**Features:**

* Number of Affected Users
* Incident Resolution Time (Hours)
* Attack Type
* Target Industry
* Attack Source
* Security Vulnerability Type
* Interaction Feature: `AttackType_TargetIndustry`
* Users_per_Hour, Log_Users

_Post-preprocessing, 25â€“65 features are used depending on the model._

---

## ğŸ”„ End-to-End ML Pipeline

1. **Feature Selection & Binary Target Creation** â€“ Define binary target using high-loss threshold.  
2. **Preprocessing Pipeline** â€“ Scale numeric features, encode categoricals, split train/test datasets.  
3. **Baseline Model Training** â€“ RandomForest, ExtraTrees, XGBoost, LightGBM, CatBoost.  
4. **Hyperparameter Tuning & Probability Cutoff** â€“ Optimize models and tune probability thresholds.  
5. **Baseline vs Model Comparison** â€“ Evaluate improvements in Accuracy, Macro F1, Macro Recall.  
6. **Feature Importance & Analysis** â€“ Aggregate expanded features to main business-level features.  
7. **Model Benchmarking** â€“ Train, test, and cross-validate models; compare metrics and inference time.  
8. **Deployment** â€“ Save best model (`production_model.joblib`) and implement prediction workflow.

---

## ğŸ§  Feature Engineering & Preprocessing

* Handle missing values (median for numeric, mode for categorical)  
* Create interaction features (`AttackType_TargetIndustry`)  
* Scale numeric variables; one-hot encode categoricals  
* Train/test split: 4:1 ratio  
* Binary target defined via high-loss threshold  

---

## ğŸ§ª Machine Learning Models Used

* Random Forest  
* Extra Trees  
* XGBoost  
* LightGBM  
* CatBoost  

---

## ğŸ” Evaluation Metrics

* Accuracy  
* Precision  
* Recall  
* Macro F1  
* ROC-AUC  
* Inference Time per Sample  

---

## ğŸ“Š Results Summary

**Model Comparison (Step 5â€“8)**

| Model       | Accuracy | Macro F1 | Macro Recall |
|------------|---------|----------|-------------|
| CatBoost    | 0.545   | 0.527    | 0.531       |
| ExtraTrees  | 0.485   | 0.480    | 0.480       |
| LightGBM    | 0.530   | 0.510    | 0.515       |
| RandomForest| 0.523   | 0.517    | 0.517       |
| XGBoost     | 0.530   | 0.530    | 0.532       |

**Best Model:** CatBoost (based on Macro F1)  

**Key Features Driving Predictions:**

* Number of Affected Users  
* Incident Resolution Time (Hours)  
* `AttackType_TargetIndustry`  
* Attack Source  

---

## ğŸ›  Handling High Variance & Bias

* Tuned regularization parameters (`l2_leaf_reg`, `max_depth`, etc.)  
* Cross-validation for robust performance  
* Feature aggregation to reduce noise  
* Avoided overfitting by limiting model complexity  

---

## ğŸš€ Deployment

* Production model saved: `deployment/production_model.joblib`  
* Prediction workflow supports new incident data using preprocessor and probability cutoff  
* Fully reproducible pipeline for operational use  

---

## ğŸ“ Project Structure

```text
CyberThreats_FinancialLoss_Prediction_ML/
â”‚â”€â”€ data/
â”‚   â”œâ”€â”€ raw/          # Original dataset CSVs
â”‚   â”œâ”€â”€ interim/      # Cleaned & selected features
â”‚   â””â”€â”€ processed/    # Step-wise processed data
â”‚
â”‚â”€â”€ notebooks/        # Step 1 â†’ Step 8 notebooks
â”‚â”€â”€ models/           # Trained models (.joblib)
â”‚â”€â”€ reports/          # eda
â”‚â”€â”€ README.md         # Project documentation

```
---

## ğŸŒ Business & Social Impact

* Cybersecurity risk assessment  
* High-loss incident forecasting  
* Incident response prioritization  
* Industry-specific risk profiling  
* Data-driven investment for improved cybersecurity  

---

## ğŸŒŸ Highlights

* âœ… End-to-end ML pipeline: preprocessing â†’ modeling â†’ deployment  
* ğŸ“ˆ CatBoost: Macro F1 = 0.527, ROC-AUC = 0.535  
* ğŸ” Key features identified for interpretability  
* âš™ï¸ Production-ready with probability thresholding  
* ğŸ” Fully reproducible and transparent workflow  

---

## ğŸ›  Tech Stack

* Python 3.10+  
* Pandas, NumPy  
* Scikit-Learn  
* CatBoost, LightGBM, XGBoost  
* Matplotlib, Seaborn  
* Jupyter Notebook  

---

## ğŸ“„ License

Educational & learning purposes only; not for commercial use.

---

## ğŸ‘¤ Author

Nithushan Uthayarasa
